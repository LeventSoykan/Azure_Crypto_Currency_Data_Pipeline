{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47ceaff-0b38-4f34-b2e8-17793779ff5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GOLD LAYER TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74855748-93af-4e57-9f39-f3c8703b66d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CONNECT DATA LAKE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b98e821-1aa4-401a-8f41-eaf0e00663b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<ms-entra-app-id>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", \"<secret-value>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab9734d-5b16-4f50-9f68-ba422250dde6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### IMPORT MODULES AND VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87797370-d8e6-4ba8-8df6-f65dee0f89cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, avg, sum as sum_, abs as abs_, when\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "482ea781-8cd2-4fda-8a2a-d2504ca36a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MAKE TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5d3b7b-7cda-4a4e-8cba-4d2068b33a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing btc...\n",
      "‚úÖ btc written to gold layer.\n",
      "Processing eth...\n",
      "‚úÖ eth written to gold layer.\n",
      "Processing doge...\n",
      "‚úÖ doge written to gold layer.\n",
      "Processing xrp...\n",
      "‚úÖ xrp written to gold layer.\n",
      "Processing usdt...\n",
      "‚úÖ usdt written to gold layer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"StockDataPipeline\").getOrCreate()\n",
    "\n",
    "for crypto in ['btc', 'eth', 'doge', 'xrp', 'usdt']:\n",
    "    print(f\"Processing {crypto}...\")\n",
    "\n",
    "    silver_path = f\"abfss://silver@<storage-account>.dfs.core.windows.net/crypto/{crypto}.csv\"\n",
    "    gold_path = f\"abfss://gold@<storage-account>.dfs.core.windows.net/crypto/{crypto}.parquet\"\n",
    "    df = spark.read.option(\"header\", \"true\").csv(silver_path)\n",
    "\n",
    "    # Cast types\n",
    "    df = df.withColumn(\"date\", col(\"date\").cast(\"date\")) \\\n",
    "           .withColumn(\"close\", col(\"close\").cast(\"float\")) \\\n",
    "           .withColumn(\"volume\", col(\"volume\").cast(\"int\"))\n",
    "\n",
    "    # Define window\n",
    "    w = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "\n",
    "    # Add enrichments\n",
    "    df = df.withColumn(\"prev_close\", lag(\"close\", 1).over(w))\n",
    "    df = df.withColumn(\"pct_change\", (col(\"close\") - col(\"prev_close\")) / col(\"prev_close\"))\n",
    "    df = df.withColumn(\"rolling_avg_7d\", avg(\"close\").over(w.rowsBetween(-6, 0)))\n",
    "    df = df.withColumn(\"rolling_volume_7d\", sum_(\"volume\").over(w.rowsBetween(-6, 0)))\n",
    "    df = df.withColumn(\"price_spike_flag\", when(\n",
    "        abs_(col(\"close\") - col(\"rolling_avg_7d\")) / col(\"rolling_avg_7d\") > 0.10, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Drop temporary column\n",
    "    df = df.drop(\"prev_close\")\n",
    "\n",
    "    # Write to gold layer (overwrite or append depending on your preference)\n",
    "    df.write.mode(\"overwrite\").parquet(gold_path)\n",
    "    \n",
    "    print(f\"‚úÖ {crypto} written to gold layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d508244-0946-4832-baaf-f7081ca47c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Combining all crypto into one DataFrame...\n",
      "‚úÖ Merged CSV written to temporary folder.\n"
     ]
    }
   ],
   "source": [
    "# Create public csv for PowerBI access\n",
    "from functools import reduce\n",
    "\n",
    "print(\"üîÅ Combining all crypto into one DataFrame...\")\n",
    "\n",
    "# Read each enriched parquet into a list of DataFrames\n",
    "dfs = [\n",
    "    spark.read.parquet(f\"abfss://gold@<storage-account>.dfs.core.windows.net/crypto/{crypto}.parquet\")\n",
    "    for crypto in ['btc', 'eth', 'doge', 'xrp', 'usdt']\n",
    "]\n",
    "\n",
    "# Union all DataFrames into one\n",
    "merged_df = reduce(lambda df1, df2: df1.unionByName(df2), dfs)\n",
    "\n",
    "# Write as a single CSV file to the public folder\n",
    "merged_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "    .csv(\"abfss://gold@<storage-account>.dfs.core.windows.net/public/gold_merged_temp\")\n",
    "\n",
    "print(\"‚úÖ Merged CSV written to temporary folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6d3cb5-2583-4d25-8884-0c3842bb2579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_data_gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
