{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47ceaff-0b38-4f34-b2e8-17793779ff5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SILVER LAYER TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74855748-93af-4e57-9f39-f3c8703b66d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CONNECT DATA LAKE BRONZE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b98e821-1aa4-401a-8f41-eaf0e00663b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<ms-entra-app-id>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", \"<secret-value>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ab9734d-5b16-4f50-9f68-ba422250dde6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### IMPORT MODULES AND VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87797370-d8e6-4ba8-8df6-f65dee0f89cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "482ea781-8cd2-4fda-8a2a-d2504ca36a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MAKE TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b5d3b7b-7cda-4a4e-8cba-4d2068b33a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Silver file doesn't exist. Writing full dataset.\n",
      "üìÅ Silver file doesn't exist. Writing full dataset.\n",
      "üìÅ Silver file doesn't exist. Writing full dataset.\n",
      "üìÅ Silver file doesn't exist. Writing full dataset.\n",
      "üìÅ Silver file doesn't exist. Writing full dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"StockDataPipeline\").getOrCreate()\n",
    "\n",
    "for crtpyo in ['btc', 'eth', 'doge', 'xrp', 'usdt']:\n",
    "    bronze_path = f\"abfss://bronze@<storage-account>.dfs.core.windows.net/crypto/{crtpyo}.json\"\n",
    "    silver_path = f\"abfss://silver@<storage-account>.dfs.core.windows.net/crypto/{crtpyo}.csv\"\n",
    "    crypto_json = spark.read.text(bronze_path).collect()[0][0]\n",
    "    crypto_dict = json.loads(crypto_json)\n",
    "\n",
    "    # Extract metadata\n",
    "    currency_name = crypto_dict[\"Meta Data\"][\"3. Digital Currency Name\"]\n",
    "\n",
    "    # Extract and flatten time series data\n",
    "    records = []\n",
    "    for date, values in crypto_dict[\"Time Series (Digital Currency Daily)\"].items():\n",
    "        records.append({\n",
    "            \"name\": currency_name,\n",
    "            \"date\": date,\n",
    "            \"close\": float(values[\"4. close\"]),\n",
    "            \"volume\": int(float(values[\"5. volume\"]))  # Cast to int\n",
    "        })\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    df = spark.createDataFrame(records)\n",
    "\n",
    "    # Cast date column\n",
    "    df = df.withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Check if silver CSV already exists\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    try:\n",
    "        existing_df = spark.read.option(\"header\", \"true\").csv(silver_path)\n",
    "        existing_df = existing_df.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "        # Merge: filter only new dates\n",
    "        new_df = df.join(existing_df, on=\"date\", how=\"left_anti\")\n",
    "        print(f\"üÜï Found {new_df.count()} new rows to append.\")\n",
    "        new_df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(silver_path)\n",
    "    except AnalysisException:\n",
    "        print(\"üìÅ Silver file doesn't exist. Writing full dataset.\")\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_data_silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
